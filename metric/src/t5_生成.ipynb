{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9ae79c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 307kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 990M/990M [00:02<00:00, 491MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 34.4kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 309MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 4.22MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 4.73MB/s]\n",
      "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import requests\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model\n",
    "import ast\n",
    "import re\n",
    "\n",
    "device = \"cpu\"\n",
    "model = \"google/flan-t5-base\"\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72ef8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"a green tree\"\n",
    "# prompt = \"a woman in a blue dress\"\n",
    "# prompt = \"a guitar and sings into headphones\"\n",
    "# prompt = \"the great wave\"\n",
    "# prompt = \"a cute dog\"\n",
    "# prompt = \"cute anime girl\"\n",
    "# prompt = \"moon night\"\n",
    "# prompt = \"cute cat the cat was crying\"\n",
    "# prompt = \"cute cat the cat was crying the cat cried after the\"\n",
    "prompt = \"cute cat the cat was crying the cat cried after the cat was crying the cat The cat was crying while the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8cd9fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cute cat the cat was crying the cat cried after the cat was crying the cat The cat was crying while the the cat cried the cat\n",
      "cute cat the cat was crying the cat cried after the cat was crying the cat The cat was crying while the cat cried\n",
      "cute cat the cat was crying the cat cried after the cat was crying the cat The cat was crying while the cat was crying the cat was\n"
     ]
    }
   ],
   "source": [
    "prefix = \"continue this sentence: \"\n",
    "context = prompt\n",
    "input_seq = prefix + '\"' + context + \"...\" + '\"'\n",
    "# 使用分词器进行编码\n",
    "input_ids = tokenizer.encode(input_seq, return_tensors=\"pt\").to(device)\n",
    "# input_ids.to(device)\n",
    "outputs = t5.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=6,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ").cpu()\n",
    "output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# output_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "for _ in output_str:\n",
    "    print(prompt+\" \"+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057763f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
