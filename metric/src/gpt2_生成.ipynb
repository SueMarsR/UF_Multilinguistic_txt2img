{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690207ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 147kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 46.8MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 49.6MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 42.4MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [00:01<00:00, 454MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 31.9kB/s]\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: cannot import name 'OutOfResources' from partially initialized module 'triton.runtime.autotuner' (most likely due to a circular import) (/usr/local/lib/python3.9/dist-packages/triton/runtime/autotuner.py)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import requests\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import ast\n",
    "import re\n",
    "\n",
    "device = \"cpu\"\n",
    "base_file = \"./data/eval_images_coarse-grained.csv\"\n",
    "model = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "gpt2_pipe = pipeline(\"text-generation\", model=model, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85592cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"a green tree\"\n",
    "# prompt = \"the great wave\"\n",
    "# prompt = \"a woman in a blue dress\"\n",
    "# prompt = \"a guitar and sings into headphones\"\n",
    "# prompt = \"a cute dog\"\n",
    "# prompt = \"cute anime girl\"\n",
    "# prompt = \"moon night\"\n",
    "# prompt = \"a fish\"\n",
    "# prompt = \"cute cat\"\n",
    "# prompt = \"cute cat, but I love cat's\"\n",
    "# prompt = \"cute cat, but I love cat's eyes and make her really happy\"\n",
    "# prompt = \"cute cat, but I love cat's eyes and make her really happy when she is on screen\"\n",
    "# prompt = \"cute cat, but I love cat's eyes and make her really happy when she is on screen. It takes some of that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef4e9d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'cute cat, but I love cat\\'s eyes and make her really happy when she is on screen. It takes some of that charm to be there.\"\\n'}\n",
      "{'generated_text': \"cute cat, but I love cat's eyes and make her really happy when she is on screen. It takes some of that fun.\\n\\nWhat are\"}\n",
      "{'generated_text': \"cute cat, but I love cat's eyes and make her really happy when she is on screen. It takes some of that to become a true fan.\"}\n",
      "{'generated_text': \"cute cat, but I love cat's eyes and make her really happy when she is on screen. It takes some of that to figure out what makes cats\"}\n",
      "{'generated_text': \"cute cat, but I love cat's eyes and make her really happy when she is on screen. It takes some of that to make her feel welcome.\"}\n"
     ]
    }
   ],
   "source": [
    "gpt2_ans = gpt2_pipe(\n",
    "    prompt,\n",
    "    # do_sample=True,\n",
    "    # num_return_sequences=5,num_beams=10,\n",
    "    max_new_tokens=6,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=5,\n",
    "    # skip_special_tokens=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for i, _ in enumerate(gpt2_ans):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263d600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
